{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "qivobhrkkl9",
   "metadata": {},
   "source": [
    "# Pydantic + HuggingFace + Biopython í†µí•© ì‹¤ìŠµ\n",
    "\n",
    "**í•™ìŠµëª©í‘œ**: Biopythonìœ¼ë¡œ PubMedì—ì„œ ë…¼ë¬¸ ì´ˆë¡ì„ ê°€ì ¸ì™€ì„œ HuggingFace APIì™€ Pydanticì„ ì‚¬ìš©í•´ êµ¬ì¡°í™”ëœ ë¶„ì„ì„ ìˆ˜í–‰í•œë‹¤.\n",
    "\n",
    "**êµ¬ì„±ìš”ì†Œ**:\n",
    "- Biopython Entrez: PubMed ë°ì´í„° ìˆ˜ì§‘\n",
    "- Pydantic: ë°ì´í„° êµ¬ì¡° ì •ì˜ ë° ê²€ì¦\n",
    "- HuggingFace: LLM ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ì„\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58036c8",
   "metadata": {},
   "source": [
    "### Huggingface ì˜ˆì œ ì½”ë“œ\n",
    "[ì˜ˆì œ ë§í¬](https://huggingface.co/docs/inference-providers/guides/structured-output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9ddcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/inference-providers/guides/responses-api\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "from pydantic import BaseModel\n",
    "\n",
    "load_dotenv()\n",
    "# Initialize the client\n",
    "client = InferenceClient(\n",
    "    provider=\"cerebras\",  # or use \"auto\" for automatic selection\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "# Example paper text (truncated for brevity)\n",
    "paper_text = \"\"\"\n",
    "Title: Attention Is All You Need\n",
    "\n",
    "Abstract: The dominant sequence transduction models are based on complex recurrent \n",
    "or convolutional neural networks that include an encoder and a decoder. The best \n",
    "performing models also connect the encoder and decoder through an attention mechanism. \n",
    "We propose a new simple network architecture, the Transformer, based solely on \n",
    "attention mechanisms, dispensing with recurrence and convolutions entirely...\n",
    "\"\"\"\n",
    "\n",
    "# Define the response format\n",
    "class PaperAnalysis(BaseModel):\n",
    "    title: str\n",
    "    abstract_summary: str\n",
    "\n",
    "# Convert the Pydantic model to a JSON Schema and wrap it in a dictionary\n",
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"PaperAnalysis\",\n",
    "        \"schema\": PaperAnalysis.model_json_schema(),\n",
    "        \"strict\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define your messages with a system prompt and a user prompt\n",
    "# The system prompt is a description of the task you want the model to perform\n",
    "# The user prompt is the input data you want to process\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"Extract paper title and abstract summary.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": paper_text\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generate structured output using Qwen/Qwen3-32B model\n",
    "response = client.chat_completion(\n",
    "    messages=messages,\n",
    "    response_format=response_format,\n",
    "    model=\"Qwen/Qwen3-32B\",\n",
    ")\n",
    "\n",
    "# The response is guaranteed to match your schema\n",
    "structured_data = response.choices[0].message.content\n",
    "print(structured_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4146f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from Bio import Entrez\n",
    "from huggingface_hub import InferenceClient\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "import json\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# HuggingFace í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "client = InferenceClient(\n",
    "    provider=\"cerebras\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "print(f\"Entrez Email: {Entrez.email}\")\n",
    "print(f\"HF API Key ì„¤ì •: {bool(os.environ.get('HF_TOKEN'))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b154239",
   "metadata": {},
   "source": [
    "### Pydantic ëª¨ë¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79544f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PubMed ë…¼ë¬¸ ì •ë³´ë¥¼ ë‹´ëŠ” Pydantic ëª¨ë¸\n",
    "class PubMedArticle(BaseModel):\n",
    "    pmid: str\n",
    "    title: str\n",
    "    abstract: str\n",
    "    authors: List[str]\n",
    "    journal: str\n",
    "    pub_year: Optional[str] = None\n",
    "\n",
    "# HuggingFaceë¡œ ë¶„ì„í•  êµ¬ì¡°í™”ëœ ì¶œë ¥ ëª¨ë¸\n",
    "class PaperAnalysis(BaseModel):\n",
    "    \"\"\"ë…¼ë¬¸ ë¶„ì„ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”í•˜ëŠ” ëª¨ë¸\"\"\"\n",
    "    title: str = Field(description=\"ë…¼ë¬¸ ì œëª©\")\n",
    "    abstract_summary: str = Field(description=\"ì´ˆë¡ ìš”ì•½ (2-3ë¬¸ì¥)\")\n",
    "    key_findings: List[str] = Field(description=\"ì£¼ìš” ë°œê²¬ì‚¬í•­ ëª©ë¡\")\n",
    "    methodology: str = Field(description=\"ì‚¬ìš©ëœ ì—°êµ¬ ë°©ë²•ë¡ \")\n",
    "    significance: str = Field(description=\"ì—°êµ¬ì˜ ì¤‘ìš”ì„± ë° ì˜í–¥\")\n",
    "    keywords: List[str] = Field(description=\"í•µì‹¬ í‚¤ì›Œë“œ 5ê°œ ì´í•˜\")\n",
    "\n",
    "print(f\"PubMedArticle ìŠ¤í‚¤ë§ˆ: {PubMedArticle.model_json_schema()}\")\n",
    "print(f\"\\nPaperAnalysis ìŠ¤í‚¤ë§ˆ: {PaperAnalysis.model_json_schema()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9608864",
   "metadata": {},
   "source": [
    "### Biopythonìœ¼ë¡œ PubMed ë°ì´í„° ìˆ˜ì§‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d6129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pubmed_articles(search_term: str, max_results: int = 5) -> List[PubMedArticle]:\n",
    "    \"\"\"PubMedì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  PubMedArticle ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\"\"\"\n",
    "    articles = []\n",
    "    \n",
    "    try:\n",
    "        # 1. PubMedì—ì„œ ê²€ìƒ‰\n",
    "        print(f\"'{search_term}' ê²€ìƒ‰ ì¤‘...\")\n",
    "        handle = Entrez.esearch(db=\"pubmed\", term=search_term, retmax=max_results)\n",
    "        search_results = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        \n",
    "        pmids = search_results['IdList']\n",
    "        print(f\"{len(pmids)}ê°œ ë…¼ë¬¸ ë°œê²¬\")\n",
    "        \n",
    "        if not pmids:\n",
    "            return articles\n",
    "        \n",
    "        # 2. ê° ë…¼ë¬¸ì˜ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "        handle = Entrez.efetch(db=\"pubmed\", id=pmids, rettype=\"abstract\", retmode=\"xml\")\n",
    "        records = Entrez.read(handle)['PubmedArticle']\n",
    "        \n",
    "        for record in records:\n",
    "            try:\n",
    "                # ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ\n",
    "                article_info = record['MedlineCitation']['Article']\n",
    "                \n",
    "                # PMID\n",
    "                pmid = record['MedlineCitation']['PMID']\n",
    "                \n",
    "                # ì œëª©\n",
    "                title = article_info.get('ArticleTitle', 'No title')\n",
    "                \n",
    "                # ì´ˆë¡\n",
    "                abstract_text = \"\"\n",
    "                if 'Abstract' in article_info:\n",
    "                    abstract = article_info['Abstract']\n",
    "                    if 'AbstractText' in abstract:\n",
    "                        abstract_sections = abstract['AbstractText']\n",
    "                        if isinstance(abstract_sections, list):\n",
    "                            # êµ¬ì¡°í™”ëœ ì´ˆë¡\n",
    "                            abstract_parts = []\n",
    "                            for section in abstract_sections:\n",
    "                                if isinstance(section, dict):\n",
    "                                    label = section.get('@Label', '')\n",
    "                                    text = section.get('#text', str(section))\n",
    "                                    if label:\n",
    "                                        abstract_parts.append(f\"{label}: {text}\")\n",
    "                                    else:\n",
    "                                        abstract_parts.append(text)\n",
    "                                else:\n",
    "                                    abstract_parts.append(str(section))\n",
    "                            abstract_text = \" \".join(abstract_parts)\n",
    "                        else:\n",
    "                            abstract_text = str(abstract_sections)\n",
    "                \n",
    "                # ì €ìë“¤\n",
    "                authors = []\n",
    "                if 'AuthorList' in article_info:\n",
    "                    for author in article_info['AuthorList']:\n",
    "                        if isinstance(author, dict):\n",
    "                            if 'LastName' in author and 'ForeName' in author:\n",
    "                                authors.append(f\"{author['ForeName']} {author['LastName']}\")\n",
    "                            elif 'CollectiveName' in author:\n",
    "                                authors.append(author['CollectiveName'])\n",
    "                \n",
    "                # ì €ë„\n",
    "                journal = article_info.get('Journal', {}).get('Title', 'Unknown journal')\n",
    "                \n",
    "                # ë°œí–‰ì—°ë„\n",
    "                pub_year = None\n",
    "                if 'Journal' in article_info and 'JournalIssue' in article_info['Journal']:\n",
    "                    pub_date = article_info['Journal']['JournalIssue'].get('PubDate', {})\n",
    "                    pub_year = pub_date.get('Year')\n",
    "                \n",
    "                # PubMedArticle ê°ì²´ ìƒì„±\n",
    "                article = PubMedArticle(\n",
    "                    pmid=str(pmid),\n",
    "                    title=title,\n",
    "                    abstract=abstract_text,\n",
    "                    authors=authors[:5],  # ìµœëŒ€ 5ëª…ì˜ ì €ìë§Œ\n",
    "                    journal=journal,\n",
    "                    pub_year=pub_year\n",
    "                )\n",
    "                \n",
    "                articles.append(article)\n",
    "                print(f\"PMID {pmid}: {title[:50]}...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"ë…¼ë¬¸ íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "                continue\n",
    "        \n",
    "        handle.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"PubMed ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# ì‹¤ì œ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "search_query = '\"machine learning\" AND bioinformatics AND 2024[DP]'\n",
    "pubmed_articles = fetch_pubmed_articles(search_query, max_results=3)\n",
    "\n",
    "print(f\"\\nìˆ˜ì§‘ ì™„ë£Œ: {len(pubmed_articles)}ê°œ ë…¼ë¬¸\")\n",
    "for i, article in enumerate(pubmed_articles):\n",
    "    print(f\"\\n{i+1}. PMID: {article.pmid}\")\n",
    "    print(f\"   ì œëª©: {article.title[:80]}...\")\n",
    "    print(f\"   ì €ì: {', '.join(article.authors[:3])}\")\n",
    "    print(f\"   ì €ë„: {article.journal}\")\n",
    "    print(f\"   ì´ˆë¡ ê¸¸ì´: {len(article.abstract)} ë¬¸ì\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eba13d",
   "metadata": {},
   "source": [
    "### HuggingFaceë¡œ êµ¬ì¡°í™”ëœ ë…¼ë¬¸ ë¶„ì„\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76d164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_paper_with_hf(article: PubMedArticle) -> PaperAnalysis:\n",
    "    \"\"\"HuggingFace APIë¥¼ ì‚¬ìš©í•´ ë…¼ë¬¸ì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë¶„ì„\"\"\"\n",
    "    \n",
    "    # Pydantic ëª¨ë¸ì„ JSON Schemaë¡œ ë³€í™˜\n",
    "    response_format = {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"PaperAnalysis\",\n",
    "            \"schema\": PaperAnalysis.model_json_schema(),\n",
    "            \"strict\": True,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    # ë…¼ë¬¸ í…ìŠ¤íŠ¸ ì¤€ë¹„\n",
    "    paper_text = f\"\"\"\n",
    "    Title: {article.title}\n",
    "    \n",
    "    Authors: {', '.join(article.authors)}\n",
    "    \n",
    "    Journal: {article.journal} ({article.pub_year})\n",
    "    \n",
    "    Abstract: {article.abstract}\n",
    "    \"\"\"\n",
    "    \n",
    "    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"\n",
    "            ë‹¹ì‹ ì€ ìƒë¬¼ì •ë³´í•™ ë° ê¸°ê³„í•™ìŠµ ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. \n",
    "            ì£¼ì–´ì§„ ë…¼ë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ í•­ëª©ë“¤ì„ ì¶”ì¶œí•˜ì„¸ìš”:\n",
    "            \n",
    "            1. ë…¼ë¬¸ ì œëª© (ì •í™•íˆ)\n",
    "            2. ì´ˆë¡ ìš”ì•½ (2-3ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©)\n",
    "            3. ì£¼ìš” ë°œê²¬ì‚¬í•­ (êµ¬ì²´ì ì¸ ê²°ê³¼ë“¤)\n",
    "            4. ì—°êµ¬ ë°©ë²•ë¡  (ì‚¬ìš©ëœ ê¸°ë²•ì´ë‚˜ ì ‘ê·¼ë²•)\n",
    "            5. ì—°êµ¬ì˜ ì¤‘ìš”ì„± ë° ì˜í–¥\n",
    "            6. í•µì‹¬ í‚¤ì›Œë“œ (5ê°œ ì´í•˜)\n",
    "            \n",
    "            ëª¨ë“  ì‘ë‹µì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , êµ¬ì¡°í™”ëœ JSON í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”.\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": paper_text\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # HuggingFace API í˜¸ì¶œ\n",
    "        print(f\"PMID {article.pmid} ë¶„ì„ ì¤‘...\")\n",
    "        response = client.chat_completion(\n",
    "            messages=messages,\n",
    "            response_format=response_format,\n",
    "            model=\"Qwen/Qwen3-32B\",\n",
    "            max_tokens=3000,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        # JSON ì‘ë‹µ íŒŒì‹±\n",
    "        structured_output = response.choices[0].message.content\n",
    "        analysis_data = json.loads(structured_output)\n",
    "        # Pydantic ëª¨ë¸ë¡œ ê²€ì¦ ë° ë³€í™˜\n",
    "        analysis = PaperAnalysis(**analysis_data)\n",
    "        \n",
    "        print(f\"ë¶„ì„ ì™„ë£Œ: {analysis.title[:50]}...\")\n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ë¶„ì„ ì˜¤ë¥˜: {e}\")\n",
    "        # ê¸°ë³¸ê°’ ë°˜í™˜\n",
    "        return PaperAnalysis(\n",
    "            title=article.title,\n",
    "            abstract_summary=\"ë¶„ì„ ì‹¤íŒ¨\",\n",
    "            key_findings=[\"ë¶„ì„ì„ ì™„ë£Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"],\n",
    "            methodology=\"ì•Œ ìˆ˜ ì—†ìŒ\",\n",
    "            significance=\"ë¶„ì„ ì‹¤íŒ¨\",\n",
    "            keywords=[\"ì˜¤ë¥˜\"]\n",
    "        )\n",
    "\n",
    "# ì²« ë²ˆì§¸ ë…¼ë¬¸ ë¶„ì„\n",
    "if pubmed_articles:\n",
    "    first_article = pubmed_articles[0]\n",
    "    print(f\"\\në¶„ì„í•  ë…¼ë¬¸: {first_article.title[:60]}...\")\n",
    "    print(f\"ì´ˆë¡ ë¯¸ë¦¬ë³´ê¸°: {first_article.abstract[:200]}...\\n\")\n",
    "    \n",
    "    analysis_result = analyze_paper_with_hf(first_article)\n",
    "    \n",
    "    print(\"\\n=== ë¶„ì„ ê²°ê³¼ ===\")\n",
    "    print(f\"ì œëª©: {analysis_result.title}\")\n",
    "    print(f\"\\nìš”ì•½: {analysis_result.abstract_summary}\")\n",
    "    print(f\"\\nì£¼ìš” ë°œê²¬ì‚¬í•­:\")\n",
    "    for i, finding in enumerate(analysis_result.key_findings, 1):\n",
    "        print(f\"  {i}. {finding}\")\n",
    "    print(f\"\\në°©ë²•ë¡ : {analysis_result.methodology}\")\n",
    "    print(f\"\\nì¤‘ìš”ì„±: {analysis_result.significance}\")\n",
    "    print(f\"\\ní‚¤ì›Œë“œ: {', '.join(analysis_result.keywords)}\")\n",
    "else:\n",
    "    print(\"ë¶„ì„í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analysis_result.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d69057",
   "metadata": {},
   "source": [
    "## ğŸ¤—ìˆ˜ê³  ë§ìœ¼ì…¨ìŠµë‹ˆë‹¤ğŸ¤—"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
